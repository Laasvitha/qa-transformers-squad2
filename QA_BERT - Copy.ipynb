{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c0dfb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: evaluate in /home/student/.local/lib/python3.10/site-packages (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/student/.local/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: requests in /home/student/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/student/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddc24115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: tqdm in /home/student/.local/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: scikit-learn in /home/student/.local/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/student/.local/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/student/.local/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a48c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa15361f682243d48725ea3293c8a6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b22307b4f8d46778f1d9762252665d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"train-v2.0.json\",\n",
    "    \"validation\": \"dev-v2.0.json\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ab4013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_squad(dataset_split):\n",
    "    context_list = []\n",
    "    question_list = []\n",
    "    answer_list = []\n",
    "    id_list = []\n",
    "\n",
    "    for article in dataset_split:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                qid = qa['id']\n",
    "\n",
    "                if qa['is_impossible']:\n",
    "                    answer = {\"text\": \"\", \"answer_start\": 0}\n",
    "                else:\n",
    "                    answer = qa['answers'][0]  # Take first answer\n",
    "\n",
    "                context_list.append(context)\n",
    "                question_list.append(question)\n",
    "                answer_list.append(answer)\n",
    "                id_list.append(qid)\n",
    "\n",
    "    return {\n",
    "        \"context\": context_list,\n",
    "        \"question\": question_list,\n",
    "        \"answers\": answer_list,\n",
    "        \"id\": id_list\n",
    "    }\n",
    "\n",
    "train_data = flatten_squad(dataset[\"train\"])\n",
    "val_data = flatten_squad(dataset[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2183ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baf18f53a4345148f5e10049bd715f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c78404e0baf499b9db2b7452c441b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6213eb3167ce42a38bfbc83873dbd57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e44c8fb4fd4fe2b9e9210011e4c048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378389a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(contexts, questions, answers):\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        sample_idx = sample_mapping[i]  # this is the index to the original example\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"]\n",
    "        end_char = start_char + len(answer[\"text\"])\n",
    "\n",
    "        # Find start and end token indices\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        # Handle no-answer case\n",
    "        if answer[\"text\"] == \"\":\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                for idx in range(token_start_index, token_end_index + 1):\n",
    "                    if offsets[idx][0] <= start_char and offsets[idx][1] >= start_char:\n",
    "                        start_pos = idx\n",
    "                    if offsets[idx][0] <= end_char and offsets[idx][1] >= end_char:\n",
    "                        end_pos = idx\n",
    "                start_positions.append(start_pos)\n",
    "                end_positions.append(end_pos)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dbb2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = preprocess_data(train_data[\"context\"], train_data[\"question\"], train_data[\"answers\"])\n",
    "val_encodings = preprocess_data(val_data[\"context\"], val_data[\"question\"], val_data[\"answers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278d1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = QADataset(train_encodings)\n",
    "val_dataset = QADataset(val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9de98c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc73ab1c053d4abb9fbb9bf9f06ad5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fbcaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-qa-checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0269a503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af73a8d1f8094a78b7670b8d60750648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cb1a4b4a2244ceaccd6fcf0b1fa0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "squad_metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    return squad_metric.compute(\n",
    "        predictions=pred.predictions,\n",
    "        references=pred.label_ids\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba4cd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b995885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32940' max='32940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32940/32940 5:54:58, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.925600</td>\n",
       "      <td>1.004801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.566800</td>\n",
       "      <td>1.214311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32940, training_loss=0.9286204977330832, metrics={'train_runtime': 21299.2532, 'train_samples_per_second': 12.372, 'train_steps_per_second': 1.547, 'total_flos': 5.164033933049242e+16, 'train_loss': 0.9286204977330832, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6453d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizerFast, pipeline\n",
    "\n",
    "model_path = \"./bert-qa-checkpoints/checkpoint-32940\"\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7dc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tmp-eval\",    # Can be any temp folder\n",
    "    per_device_eval_batch_size=8\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89ae2214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import collections\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Get raw predictions (start_logits, end_logits)\n",
    "raw_predictions = trainer.predict(val_dataset)\n",
    "start_logits, end_logits = raw_predictions.predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5af92db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(\n",
    "    examples,\n",
    "    features,\n",
    "    raw_predictions,\n",
    "    n_best_size=20,\n",
    "    max_answer_length=30\n",
    "):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "\n",
    "    for i, sample_idx in enumerate(features[\"overflow_to_sample_mapping\"]):\n",
    "        features_per_example[sample_idx].append(i)\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    for example_index, example_id in enumerate(examples[\"id\"]):\n",
    "        context = examples[\"context\"][example_index]\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None\n",
    "        valid_answers = []\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            offset_mapping = features[\"offset_mapping\"][feature_index]\n",
    "            input_ids = features[\"input_ids\"][feature_index]\n",
    "\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or feature_null_score < min_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-1: -n_best_size - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1: -n_best_size - 1: -1].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append({\n",
    "                        \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                        \"text\": context[start_char: end_char]\n",
    "                    })\n",
    "\n",
    "        if valid_answers:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            best_answer = {\"text\": \"\"}\n",
    "\n",
    "        if min_null_score is not None and (\"score\" not in best_answer or min_null_score < best_answer[\"score\"]):\n",
    "            predictions[example_id] = \"\"\n",
    "        else:\n",
    "            predictions[example_id] = best_answer[\"text\"]\n",
    "\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98e6b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings = tokenizer(\n",
    "    val_data[\"question\"],\n",
    "    val_data[\"context\"],\n",
    "    truncation=\"only_second\",\n",
    "    max_length=384,\n",
    "    stride=128,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "\n",
    "val_encodings[\"overflow_to_sample_mapping\"] = val_encodings.pop(\"overflow_to_sample_mapping\")\n",
    "val_encodings[\"offset_mapping\"] = val_encodings.pop(\"offset_mapping\")\n",
    "val_encodings[\"input_ids\"] = val_encodings[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "207fdf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = postprocess_qa_predictions(val_data, val_encodings, (start_logits, end_logits))\n",
    "\n",
    "# Save to predictions.json\n",
    "import json\n",
    "with open(\"predictions.json\", \"w\") as f:\n",
    "    json.dump(predictions, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0a91111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"exact\": 50.07159100480081,\r\n",
      "  \"f1\": 50.07159100480081,\r\n",
      "  \"total\": 11873,\r\n",
      "  \"HasAns_exact\": 0.0,\r\n",
      "  \"HasAns_f1\": 0.0,\r\n",
      "  \"HasAns_total\": 5928,\r\n",
      "  \"NoAns_exact\": 100.0,\r\n",
      "  \"NoAns_f1\": 100.0,\r\n",
      "  \"NoAns_total\": 5945\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate-v2.0.py dev-v2.0.json predictions.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8415b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizerFast, pipeline\n",
    "\n",
    "model_path = \"./bert-qa-checkpoints/checkpoint-32940\"\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path).to(\"cuda\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f97ce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Paris.\n"
     ]
    }
   ],
   "source": [
    "def get_answer(question, context):\n",
    "    return qa_pipeline({'question': question, 'context': context})['answer']\n",
    "\n",
    "context = \"The Eiffel Tower is located in Paris.\"\n",
    "question = \"Where is the Eiffel Tower?\"\n",
    "\n",
    "print(\"Answer:\", get_answer(question, context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27e07436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Eiffel Tower\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Harry Potter?\"\n",
    "\n",
    "print(\"Answer:\", get_answer(question, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a227160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Quick, name a book series more popular than Harry Potter. ... Maybe Game of Thrones? Truth is, the popularity of the Potter-verse is pretty much unmatched. But who's surprised? The story has it all: lovable heroes, terrifying villains, frickin' magic... And to top it all off, enough twists, turns and itty-bitty details to rival Hogwarts itself. No wonder, then, that the series is as popular today as it was back when The Philosopher's Stone (that's Sorcerer's Stone for the Americans) first introduced us to mugwumps and muggles more than 20 years ago. We've been obsessed with the wizarding world of Harry P for two decades now, and counting. But how much do you know about Harry & Co? Do you know what inspired J.K. Rowling to make Quidditch? Or which hex Snape invented himself? Or how about the lifespan of a Basilisk? The answers to these questions and more are collected together here, in one handy, exhaustive guide. Look no further, Potter Pal. Here's everything you ever need to know about the Harry Potter Universe. Harry Potter Universe  Facts 1. Try, Try, And Try AgainWhy is it that all the greats are rejected at first? Between Walt Disney, Oprah Winfrey, even freaking Elvis... are we trying to not be entertained? Case in point: J.K. Rowling’s first Harry Potter manuscript was rejected by 12 publishing houses, before being picked up by Bloomsbury. It got so bad for a while, she even tried submitting her magical magnum opus under a pen name:  Robert Galbraith. After that tactic failed, she almost gave up. She even complained to a friend, saying, They don’t even want me in a beard! But look at her now! Those publishers must be feeling more regret than the wizard who tried kissing a Blast-Ended Skrewt.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a51d56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: J.K. Rowling’\n"
     ]
    }
   ],
   "source": [
    "question = \"Who wrote Harry Potter?\"\n",
    "\n",
    "print(\"Answer:\", get_answer(question, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1fcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
